{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bb2cd1",
   "metadata": {},
   "source": [
    "# DS-6 Data ETL  Pipeline Using Python (Extracting, Transforming and Loading)\n",
    "\n",
    "* developing DataETL pipelines is one of the most valuable skills for DataEngineers.\n",
    "\n",
    "* Data ETL is process where data is extracted from a place, then the data is transformed in some way, and then data is loaded into a database.So ETL (Extracting, Transforming and Loading the data)\n",
    "\n",
    "* To Develop a Data ETL Pipeline  using python the first step is to collect data from data source. Let's use the FASHION -MINST\n",
    "dataset provided by the Keras library to keep things Beginner Friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237e6ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting tensorflow\n",
      "  Using cached tensorflow-2.12.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Using cached tensorflow_intel-2.12.0-cp39-cp39-win_amd64.whl (272.8 MB)\n",
      "Requirement already satisfied: setuptools in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: packaging in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.54.0-cp39-cp39-win_amd64.whl (4.1 MB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.3.0)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting jax>=0.3.15\n",
      "  Using cached jax-0.4.8-py3-none-any.whl\n",
      "Requirement already satisfied: flatbuffers>=2.0 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in e:\\andaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in e:\\andaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: scipy>=1.7 in e:\\andaconda\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.1)\n",
      "Collecting ml-dtypes>=0.0.3\n",
      "  Using cached ml_dtypes-0.1.0-cp39-cp39-win_amd64.whl (120 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\andaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\andaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\andaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\andaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in e:\\andaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\andaconda\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\andaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\andaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in e:\\andaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\andaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\andaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\andaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\andaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in e:\\andaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in e:\\andaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: opt-einsum, ml-dtypes, keras, grpcio, google-pasta, gast, astunparse, absl-py, requests-oauthlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.0 jax-0.4.8 keras-2.12.0 ml-dtypes-0.1.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.12.2 tensorflow-2.12.0 tensorflow-intel-2.12.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (e:\\andaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8eba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 10s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow.keras as keras\n",
    "(xtrain, ytrain), (xtest, ytest) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a166f9c",
   "metadata": {},
   "source": [
    "* Befor moving forward Let's have a look at the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ce4bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45810a0",
   "metadata": {},
   "source": [
    "* Now Let's Clean and Transform the data. Here we will normalize the pixel values to be between 0 to 1 and reshape the data into a 4D tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb693f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xtrain = xtrain.astype('float32') / 255\n",
    "xtest = xtest.astype('float32') / 255\n",
    "\n",
    "\n",
    "xtrain= np.reshape(xtrain, (xtrain.shape[0],28,28,1))\n",
    "xtest = np.reshape(xtest, (xtest.shape[0],28,28,1)) \n",
    "\n",
    "print(xtrain.shape)\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379c0e5",
   "metadata": {},
   "source": [
    "* Now Let's Load the data into a database, We ca nuse SQLite to create a database and load the data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a343b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('fashion_mnist.db')\n",
    "\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS images\n",
    "            (id INTEGER PRIMARY KEY  AUTOINCREMENT ,\n",
    "            image BLOB NOT NULL,\n",
    "            label INTERGER NOT NULL);''')\n",
    "\n",
    "for i in range(xtrain.shape[0]):\n",
    "    conn.execute('INSERT INTO images(image, label) VALUES (?,?)',\n",
    "                [sqlite3.Binary(xtrain[i]), ytrain[i]])\n",
    "                 \n",
    "conn.commit()\n",
    "for i in range(xtest.shape[0]):\n",
    "                 conn.execute('INSERT INTO images(image, label) VALUES (?, ?)',\n",
    "                             [sqlite3.Binary(xtest[i]), ytest[i]])\n",
    "        \n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7ec0c",
   "metadata": {},
   "source": [
    "In Above Code: \n",
    "    \n",
    "    1. The First line imports a library called sqlite3, which allows us to work with SQLite databases in Python;\n",
    "    2. We then create a connection to the database;\n",
    "    3. next, we create a table in the databse called \"imagees\";\n",
    "    4. We use a loop to llop thorugh each image in the training data and insert it into the \"images\" table (along with the labels);\n",
    "    5. We use the commit() method to save the  the changes we made to the database;\n",
    "    6. We then use another loop to loop through each image in the test data and insert it into the \"images\"table(along with Lables)\n",
    "    7. We use the commit() method again to save the changes we made to the database;\n",
    "    8. Finally we close the connection to the database;\n",
    "    \n",
    "    \n",
    "    \n",
    "* So this is how we can create a DataETL pipeline using python. Our ETL pipeline takes the Fashion MNIST dataset and stores it in an SQLite databse so that we can easily access and manipulate the data later.\n",
    "\n",
    "\n",
    "### Now This is how you can read the data you stored on the SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84a4403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  sqlite3\n",
    "conn = sqlite3.connect(\"fashion_mnist.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('SELECT * FROM images')\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_sql_query('SELECT * FROM images', conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b7de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
